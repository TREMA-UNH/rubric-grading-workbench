# DL20 Participant Leaderboard: Autograder-Qrels


**Results on DL20 autograde-qrels**, on nuggets and questions of minimum ratings 3,4, and 5 with standard error bars. Results are sorted by official leaderboard rank (denoted in right-most columns). Results are produced with `trec_eval` using `P@20` metric. The current implementation does not support standard error bars.  Leaderboard-based rank correlation is given at the bottom of the table.  "\_overall\_" denotes a theoretical upper bound score achieved across all graded passages.

| method | **nugget-3** | **nugget-4** | **nugget-5** | **question-3** | **question-4** | **question-5** | **official rank** |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| pash\_r3 | **0.95** | **0.815** | **0.196** | **0.795** | **0.768** | **0.322** | **1** |
| pash\_r2 | **0.949** | **0.814** | **0.198** | **0.793** | **0.766** | **0.322** | **2** |
| pash\_f3 | **0.95** | **0.82** | **0.194** | **0.798** | **0.769** | **0.331** | **3** |
| pash\_f1 | **0.95** | **0.82** | **0.194** | **0.797** | **0.768** | **0.33** | **4** |
| pash\_f2 | **0.95** | **0.818** | **0.196** | **0.797** | **0.771** | **0.33** | **5** |
| p\_d2q\_bm25\_duo | **0.961** | **0.813** | **0.198** | **0.791** | **0.76** | **0.33** | **6** |
| p\_d2q\_rm3\_duo | **0.957** | **0.813** | **0.198** | **0.79** | **0.761** | **0.334** | **7** |
| p\_bm25rm3\_duo | **0.955** | **0.818** | **0.201** | **0.771** | **0.742** | **0.332** | **8** |
| CoRT-electra | **0.936** | **0.791** | **0.186** | **0.769** | **0.746** | **0.322** | **9** |
| RMIT-Bart | **0.946** | **0.806** | **0.198** | **0.77** | **0.745** | **0.31** | **10** |
| pash\_r1 | **0.948** | **0.813** | **0.2** | **0.789** | **0.763** | **0.323** | **11** |
| NLE\_pr3 | **0.948** | **0.818** | **0.203** | **0.785** | **0.759** | **0.324** | **12** |
| pinganNLP2 | **0.953** | **0.825** | **0.203** | **0.781** | **0.758** | **0.32** | **13** |
| pinganNLP3 | **0.944** | **0.816** | **0.199** | **0.78** | **0.756** | **0.314** | **14** |
| pinganNLP1 | **0.953** | **0.825** | **0.202** | **0.781** | **0.758** | **0.319** | **15** |
| NLE\_pr2 | **0.941** | **0.807** | **0.199** | **0.79** | **0.763** | **0.322** | **16** |
| NLE\_pr1 | **0.943** | **0.8** | **0.194** | **0.777** | **0.752** | **0.319** | **17** |
| 1 | **0.922** | **0.785** | **0.194** | **0.738** | **0.712** | **0.304** | **18** |
| bigIR-BERT-R | **0.944** | **0.793** | **0.191** | **0.752** | **0.732** | **0.311** | **19** |
| fr\_pass\_roberta | **0.932** | **0.78** | **0.194** | **0.75** | **0.72** | **0.312** | **20** |
| bigIR-DCT-T5-F | **0.935** | **0.794** | **0.194** | **0.755** | **0.725** | **0.306** | **21** |
| rr-pass-roberta | **0.941** | **0.797** | **0.196** | **0.754** | **0.729** | **0.319** | **22** |
| bcai\_bertl\_pass | **0.946** | **0.823** | **0.187** | **0.759** | **0.735** | **0.315** | **23** |
| bigIR-T5-R | **0.936** | **0.799** | **0.202** | **0.757** | **0.733** | **0.315** | **24** |
| 2 | **0.92** | **0.783** | **0.184** | **0.705** | **0.678** | **0.303** | **25** |
| bigIR-T5-BERT-F | **0.941** | **0.777** | **0.182** | **0.744** | **0.717** | **0.306** | **26** |
| bigIR-T5xp-T5-F | **0.93** | **0.793** | **0.197** | **0.747** | **0.717** | **0.31** | **27** |
| nlm-ens-bst-2 | **0.95** | **0.797** | **0.19** | **0.764** | **0.746** | **0.303** | **28** |
| nlm-ens-bst-3 | **0.95** | **0.792** | **0.18** | **0.754** | **0.733** | **0.283** | **29** |
| nlm-bert-rr | **0.932** | **0.782** | **0.198** | **0.756** | **0.73** | **0.304** | **30** |
| relemb\_mlm\_0\_2 | **0.947** | **0.783** | **0.182** | **0.743** | **0.722** | **0.303** | **31** |
| nlm-prfun-bert | **0.934** | **0.77** | **0.172** | **0.726** | **0.706** | **0.286** | **32** |
| TUW-TK-Sparse | **0.919** | **0.754** | **0.173** | **0.709** | **0.685** | **0.292** | **33** |
| TUW-TK-2Layer | **0.927** | **0.777** | **0.186** | **0.735** | **0.713** | **0.295** | **34** |
| p\_d2q\_bm25 | **0.923** | **0.756** | **0.164** | **0.723** | **0.689** | **0.263** | **35** |
| p\_d2q\_bm25rm3 | **0.917** | **0.756** | **0.174** | **0.726** | **0.692** | **0.281** | **36** |
| bert\_6 | **0.922** | **0.754** | **0.178** | **0.695** | **0.668** | **0.276** | **37** |
| CoRT-bm25 | **0.904** | **0.74** | **0.157** | **0.685** | **0.652** | **0.248** | **38** |
| CoRT-standalone | **0.844** | **0.699** | **0.15** | **0.62** | **0.601** | **0.241** | **39** |
| bl\_bcai\_mdl1\_vt | **0.924** | **0.75** | **0.121** | **0.681** | **0.643** | **0.237** | **40** |
| bcai\_class\_pass | **0.919** | **0.751** | **0.143** | **0.68** | **0.65** | **0.236** | **41** |
| bl\_bcai\_mdl1\_vs | **0.906** | **0.739** | **0.136** | **0.67** | **0.639** | **0.232** | **42** |
| indri-fdm | **0.924** | **0.762** | **0.133** | **0.681** | **0.654** | **0.23** | **43** |
| terrier-InL2 | **0.925** | **0.768** | **0.128** | **0.69** | **0.658** | **0.235** | **44** |
| terrier-BM25 | **0.931** | **0.784** | **0.132** | **0.69** | **0.656** | **0.226** | **45** |
| DLH\_d\_5\_t\_25 | **0.93** | **0.751** | **0.142** | **0.694** | **0.657** | **0.234** | **46** |
| indri-lmds | **0.925** | **0.775** | **0.127** | **0.687** | **0.656** | **0.228** | **47** |
| indri-sdm | **0.919** | **0.756** | **0.134** | **0.672** | **0.643** | **0.226** | **48** |
| p\_bm25rm3 | **0.915** | **0.75** | **0.144** | **0.678** | **0.647** | **0.246** | **49** |
| p\_bm25 | **0.908** | **0.732** | **0.138** | **0.64** | **0.609** | **0.207** | **50** |
| bm25\_bert\_token | **0.893** | **0.709** | **0.135** | **0.609** | **0.585** | **0.22** | **51** |
| terrier-DPH | **0.934** | **0.788** | **0.137** | **0.668** | **0.64** | **0.226** | **52** |
| TF\_IDF\_d\_2\_t\_50 | **0.903** | **0.721** | **0.121** | **0.665** | **0.639** | **0.221** | **53** |
| small\_1k | **0.693** | **0.532** | **0.104** | **0.51** | **0.489** | **0.162** | **54** |
| med\_1k | **0.678** | **0.52** | **0.084** | **0.492** | **0.472** | **0.149** | **55** |
| DoRA\_Large\_1k | **0.665** | **0.512** | **0.091** | **0.492** | **0.471** | **0.153** | **56** |
| DoRA\_Small | **0.543** | **0.326** | **0.029** | **0.169** | **0.154** | **0.018** | **57** |
| DoRA\_Med | **0.515** | **0.291** | **0.027** | **0.15** | **0.138** | **0.018** | **58** |
| DoRA\_Large | **0.517** | **0.304** | **0.03** | **0.154** | **0.14** | **0.016** | **59** |
|  |  |  |  |  |  |  |  |
| spearman | **0.859** | **0.894** | **0.894** | **0.961** | **0.953** | **0.972** |  |
| kendall | **0.684** | **0.734** | **0.729** | **0.834** | **0.818** | **0.872** |  |
| min\_rating | **3** | **4** | **5** | **3** | **4** | **5** |  |
